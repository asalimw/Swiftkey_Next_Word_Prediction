---
title: "Swiftkey Next Word Prediction - Modelling"
author: "Willianto Asalim"
date: "01/09/2020"
output: html_document
---


```{r LoadPackages, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr) ##Load Knitr package
library(ggplot2) ##Plotting and data
library(caret) ##Load package for ML
library(dplyr) ##Data transformation package
library(quanteda)
library(ngram) ## 
library(tm)
library(RColorBrewer)
library(ggthemes)
library(gridExtra)
library(tidytext)
library(wordcloud)
library(markovchain)
library(tidyr)
library(data.table)
library(tidyr)
library(stringi)
library(stringr)
library(plotly)
```


```{r setoptions, echo=FALSE}
## Setting Global Option where echo = true so that someone will be able to read the code and results.
knitr::opts_chunk$set(echo = FALSE, results = "hold", tidy = TRUE)
```

## Load Relevant Data
```{r loadRelevantData}

load(file = "./data/clean/uniWords.rda")
load(file = "./data/clean/biWords.rda")
load(file = "./data/clean/triWords.rda")
load(file = "./data/clean/quadWords.rda")
load(file = "./data/clean/pentaWords.rda")

#load(file = "./data/sampleToken.rda")
load(file = "./data/sampleCorpus.rda")
```

## Markov Chain Fit

Using [Markov Chain](https://setosa.io/ev/markov-chains/) to predict the next word and credit to [Daniel 
Shiffman](https://www.youtube.com/watch?v=eGFJ8vugIWA&t=1064s) for his insight with text generator using Markov Chain.

Credit to "NLP: What would Shakepeare say?" blog by [Trinviam V Ganesh](https://www.r-bloggers.com/natural-language-processing-what-would-shakespeare-say/) for the guide to implement the modelling. Our fellow member Paul Ringsted in the discussion for giving the tips and pointers on how to proceed with this.

As stated by Dan Durafsky in [Interpolation](https://www.youtube.com/watch?v=naNezonMA7k) part of NLP, the best method to use for for very large N-grams like the web would be to use the "Stupid Backoff" method. 

Credit to Michael Szcepaniak from the forum for providing the   [Katz Backoff Formula](https://raw.githubusercontent.com/MichaelSzczepaniak/DSSCapstoneMentor/master/kbot_complete_hand_written_example.pdf)

```{r markovChain}

#' Create Ngram Matching Functions
bigram.Pred <- function(input_words){
                    num <- length(input_words)
                    filter(biWords, 
                          word1==input_words[num]) %>% 
                    slice_max(5, n = 5) %>%
                    filter(row_number() <= 1L) %>%
                    select(num_range("word", 2)) %>%
                    as.character() -> out
                    ifelse(out =="character(0)", "?", return(out))
}

trigram.Pred <- function(input_words){
                    num <- length(input_words)
                    filter(triWords, 
                            word1==input_words[num-1], 
                            word2==input_words[num])  %>% 
                    slice_max(5, n = 5) %>%
                    filter(row_number() <= 1L) %>%
                    select(num_range("word", 3)) %>%
                    as.character() -> out
                    ifelse(out=="character(0)", bigram.Pred(input_words), return(out))
}

quadgram.Pred <- function(input_words){
                    num <- length(input_words)
                    filter(quadWords, 
                            word1==input_words[num-2], 
                            word2==input_words[num-1], 
                            word3==input_words[num])  %>% 
                    slice_max(5, n = 5) %>%
                    filter(row_number() <= 1L) %>%
                    select(num_range("word", 4)) %>%
                    as.character() -> out
                    ifelse(out=="character(0)", trigram.Pred(input_words), return(out))
            
}


pentagram.Pred <- function(input_words){
                    num <- length(input_words)
                    filter(pentaWords, 
                            word1==input_words[num-3], 
                            word2==input_words[num-2], 
                            word3==input_words[num-1],
                            word4==input_words[num])  %>% 
                    slice_max(5, n = 5) %>%
                    filter(row_number() <= 1L) %>%
                    select(num_range("word", 5)) %>%
                    as.character() -> out
                    ifelse(out=="character(0)", quadgram.Pred(input_words), return(out))
}
```


```{r Backoff}

predictWord <- function(input){
  # Create a data table
  input <- data.frame(text = input)
  
  # Clean the Input
  replace_reg <- "[^[:alpha:][:space:]]*"
  input <- input %>%
    mutate(text = str_replace_all(text, replace_reg, ""))
  
  # remove the special characters that might indicate "Twitter" or other social media conventions.
  input <- input %>%
              stri_replace_all_regex("[\\p{p}\\p{S}]", "") %>%   #remove all punctuation and symbols
              stri_replace_all_regex("(\\b)_(\\w+)", "$1$2") %>% #how to remove the leading _
              stri_replace_all_regex("\\d", "") #remove all digits
    
  # Find word count, separate words, lower case
  input_count <- str_count(input, boundary("word"))
  input_words <- unlist(str_split(input, boundary("word")))
  input_words <- tolower(input_words)
  
  # Call the matching functions
  out <- ifelse(input_count == 1, bigram.Pred(input_words), 
              ifelse (input_count == 2, trigram.Pred(input_words), 
                      ifelse(input_count == 3, quadgram.Pred(input_words), pentagram.Pred(input_words))))
                              
  # Output
  return(out)
}
```


```{r backoff_AltStyle}
bigram_pred <- function(input_token, freq_matrix){
    words_temp <- input_token
    
    # based on word, choose the bigram with highest frequency
    # (since our data frame is sorted by frequency, the first one will have
    # the highest frequency)
    
    nextword <- freq_matrix[word1 == input_token]
    
    if(length(nextword[,frequency]) == 0){
        nextword = 0
    }
    # return next word
    else{
        nextword <- arrange(nextword, desc(frequency))
        nextword <- nextword[, c("word2", "frequency")]
        colnames(nextword) <- c("Prediction", "Calculated Probability")
        nextword
    }
}

trigram_pred <- function(input_token, freq_matrix){
            
        # based on word, choose the trigram with highest frequency
        # (since our data frame is sorted by frequency, the first one will have
        # the highest frequency)
        
        nextword <- freq_matrix[word1 == input_token[1] & word2 == input_token[2]
                                ]
        if(length(nextword[,frequency]) == 0){
            nextword = 0
        }
        # return next word
        else{
            nextword <- arrange(nextword, desc(frequency))
            nextword <- nextword[, c("word3", "frequency")]
            colnames(nextword) <- c("Prediction", "Calculated Probability")
            nextword
        }
}
    
fourgram_pred <- function(input_token, freq_matrix){
    
    # based on word, choose the trigram with highest frequency
    # (since our data frame is sorted by frequency, the first one will have
    # the highest frequency)
    nextword <- freq_matrix[word1 == input_token[1] &
                            word2 == input_token[2] &
                            word3 == input_token[3]
                            ]
    if(length(nextword[,frequency]) == 0){
        nextword = 0
    }
    # return next word
    else{
        nextword <- arrange(nextword, desc(frequency))
        nextword <- nextword[, c("word4", "frequency")]
        colnames(nextword) <- c("Prediction", "Calculated Probability")
        nextword
    }
}



# backoff model

backoff_stop <- function(inputtext){
    tok <- tokens(char_tolower(inputtext),
                  remove_punct = TRUE,
#                  remove_twitter = TRUE,
                  remove_symbols = TRUE,
                  remove_url = TRUE
    )
    
    count <- length(tok$text1)
    result = 0
    
    if(count == 0){
        uniWords
    }
    else{
        
        if(count >= 3 || result == 0){
            last_three <- tail(tok$text1, 3)
            result = fourgram_pred(last_three, quadWords)
        }
        
        if(count == 2 || result == 0){
            last_two <- tail(tok$text1, 2)
            result = trigram_pred(last_two, triWords)
        }
        
        
        if(count == 1 || result == 0){
            last_one <- tail(tok$text1, 1)
            result = bigram_pred(last_one, biWords)
        }
        
        if(result == 0){
            mono
        }
        else{
            result
        }
    }
}


```


```{r test}
test1 <- "the same is"
test2 <- "#are you sure?"
test3 <- "%$#^&*^"
test4 <- "&^&^ mother i am sorry"
test5 <- "halo i am 98766 why are you &^^&^"
test6 <- "!@#3434How am787673^%"
test7 <- "how am"
test8 <- "The Lord is my"
test9 <- "Even though I walk through the valley of the shadow of"
```


```{r Quiz 3}
q1 <- "When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd"
q2 <- "Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his"
q3 <- "I'd give anything to see arctic monkeys this"
q4 <- "Talking to your mom has the same effect as a hug and helps reduce your"
q5 <- "When you were in Holland you were like 1 inch away from me but you hadn't time to take a"
q6 <- "I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the"
q7 <- "I can't deal with unsymetrical things. I can't even hold an uneven number of bags of groceries in each"
q8 <- "Every inch of you is perfect from the bottom to the"
q9 <- "Iâ€™m thankful my childhood was filled with imagination and bruises from playing"
q10 <- "I like how the same people are in almost all of Adam Sandler's"
```




References:

Language Models by Boyd-Graber - https://www.youtube.com/watch?v=4wa2WyDrgMA&list=PLTmbBSJTq9L6w3qvVt0k6POBWPR1Tu8Yk&index=69&t=0s

decontextualize by Allison Parrish - http://www.decontextualize.com/teaching/rwet/n-grams-and-markov-chains/

Web Trigrams: Visualizing Google's Tri-Gram Data by Chris Harrison - https://www.chrisharrison.net/index.php/Visualizations/WebTrigrams