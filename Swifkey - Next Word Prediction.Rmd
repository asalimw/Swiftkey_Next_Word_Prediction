---
title: "Swiftkey_Next_Word_Prediction - Milestone Report"
author: "Willianto Asalim"
date: "18/08/2020"
output: html_document
---

##### The system platform specification used:
Spec    | Description
------- | -----------------------
OS      | Windows 10 Pro - 64 bit
CPU     | AMD Ryzen 5 - 3400G (4 cores & 8 threads)
RAM     | 16GB DDR4 3000MHz
Storage | 500GB SSD - M.2 NVMe (PCIe) 
Tool    | RStudio


```{r LoadPackages, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr) ##Load Knitr package
library(ggplot2) ##Plotting and data
library(caret) ##Load package for ML
library(dplyr) ##Data transformation package
library(quanteda)
library(ngram) ## 
library(tm)
library(RColorBrewer)
library(ggthemes)
library(gridExtra)
library(tidytext)
```

```{r setoptions, echo=FALSE}
## Setting Global Option where echo = true so that someone will be able to read the code and results.
knitr::opts_chunk$set(echo = TRUE, results = "hold", tidy = TRUE)
```

# Swiftkey - Next Word Prediction

## Background

[Natural Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP) has come a long way according to [Dan Durafsky](https://youtu.be/oWsMIW-5xUc) from Stanford. It has taken over language translation and helped in solving issues such as spam detection, part of speech tagging and named entity recognition.  However the technology for dialog, question and answering, summurisation is still really hard due to ambuiguity. 

Around the world, people are spending an increasing amount of time on their mobile devices for email, social networking, banking and a whole range of other activities. But typing on mobile devices can be a serious pain. SwiftKey, our corporate partner in this capstone, builds a smart keyboard that makes it easier for people to type on their mobile devices. One cornerstone of their smart keyboard is predictive text models. When someone types:

I went to the

the keyboard presents three options for what the next word might be. For example, the three words might be gym, store, restaurant. 

In this capstone we will work on understanding and building predictive text models like those used by SwiftKey.

#### How do we approach this?

The first step in analyzing any new data set is figuring out: 
(a) what data you have and 
(b) what are the standard tools and models used for that type of data. 


#### Data:
This project uses the files named LOCALE.blogs.txt where LOCALE is the each of the four locales en_US, de_DE, ru_RU and fi_FI. The data is from a corpus called HC Corpora. The files have been language filtered but may still contain some foreign text. This is the training data to get started that will be the basis for most of the capstone. Download the data from the link below and not from external websites to start.

[HC Corpora Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)

The corpora are collected from publicly available sources by a web crawler. The crawler checks for language, so as to mainly get texts consisting of the desired language. Each entry is tagged with it's date of publication. Where user comments are included they will be tagged with the date of the main entry.

Each entry is tagged with the type of entry, based on the type of website it is collected from (e.g. newspaper or personal blog) If possible, each entry is tagged with one or more subjects based on the title or keywords of the entry (e.g. if the entry comes from the sports section of a newspaper it will be tagged with "sports" subject).In many cases it's not feasible to tag the entries (for example, it's not really practical to tag each individual Twitter entry, though I've got some ideas which might be implemented in the future) or no subject is found by the automated process, in which case the entry is tagged with a '0'.

Once the raw corpus has been collected, it is parsed further, to remove duplicate entries and split into individual lines. Approximately 50% of each entry is then deleted. 

#### Tools:
As a first step toward working on this project is to familiarize Natural Language Processing, Text Mining, and the associated tools in R. Please refer to the reference below for all the materials in regards to NLP and Text Mining. 

The main development application for this project that I will be using is [RStudio](https://rstudio.com/) and the main libraries for NLP will be [TM](https://cran.r-project.org/web/packages/tm/index.html), [Quanteda](https://cran.r-project.org/web/packages/quanteda/index.html), [Wordcloud](https://cran.r-project.org/web/packages/wordcloud/index.html) and [Tidytext](https://cran.r-project.org/web/packages/tidytext/index.html).

There are other libraries such as ggplot2 for plotting, caret for machine learning, dplyr for data manipulation and etc for this projects.

#### Modelling:
the first step in building a predictive text mining application is to build your first simple model for the relationship between words.

Given the huge scale of more than 500MB of data and It is inefficient and impractical to explore and clean all of the data. It will take a lot of computing power and time to perform all the required task to accomplish the objective. Therefore we will take a sample size from each En_US text file and combined them into a single corpora. A good maximum sample size is usually around 10% of the population, For example, in a population of 5000, 10% would be 500. In a population of 200,000, 10% would be 20,000. Therefore we will need create samples for the blogs, news and twitter dataset.

Next I will build basic n-gram model - using the exploratory analysis you performed, build a basic n-gram model for predicting the next word based on the previous 1, 2, or 3 words. Then build a model to handle unseen n-grams - in some cases people will want to type a combination of words that does not appear in the corpora. Build a model to handle cases where a particular n-gram isn't observed.

***

## Preprocessing

**Please refer to the code appendix below for the detail of the code used to perform the task.**

#### Task 1: Gathering and Cleaning Data
Download the data on the the directory specified and unzip the file at the same time. 
```{r downloadData}
# Task 1: Getting and cleaning data

# URL is the given link to download from HC Corpora dataset
data.Url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
filepath <- "./data/SwiftKey.zip" #set the location and file name of the downloaded zip file

# Create directory named data for the file to download
if (!file.exists("./data")) {
  dir.create("./data")
}

# Download and unzip the file.
if (!file.exists(filepath)){
  download.file(url, destfile=filepath, method="curl")
  unzip(zipfile=filepath, exdir="./data") #Unzip the file and store the folder in the data folder
}

# Assign English data location
loc.Blogs <- "./data/final/en_US/en_US.blogs.txt"
loc.News <- "./data/final/en_US/en_US.news.txt"
loc.Twitter <- "./data/final/en_US/en_US.twitter.txt"

# Read the data files
raw.Blogs <- readLines(loc.Blogs, encoding = "UTF-8", skipNul = TRUE)
raw.News <- readLines(loc.News, encoding = "UTF-8", skipNul = TRUE, warn = FALSE)
raw.Twitter <- readLines(loc.Twitter, encoding = "UTF-8", skipNul = TRUE)
```

#### Cleaning Data

Below is the information of the En_US files that we have successfully downloaded and unzip:
```{r checkFile}
Rprof(checking <- "./data/checking.txt") #Checking for bottleneck

MB <- 2^20 #Byte to MB conversion

# Check File Size
size <- round(c(object.size(raw.Blogs)/MB,
                object.size(raw.News)/MB,
                object.size(raw.Twitter)/MB),2)

# Number of lines in each file
lines <- c(length(raw.Blogs), 
           length(raw.News), 
           length(raw.Twitter))

# Number of characters in each file
char <- c(sum(nchar(raw.Blogs)), 
          sum(nchar(raw.News)), 
          sum(nchar(raw.Twitter)))

# Number of words
words <- c(wordcount(raw.Blogs, sep =" "), 
           wordcount(raw.News, sep =" "), 
           wordcount(raw.Twitter, sep =" "))

# Longest line
long.Line <- c(max(sapply(raw.Blogs, nchar)),
               max(sapply(raw.News, nchar)),
               max(sapply(raw.Twitter, nchar)))

raw.Info <- cbind(size, lines, char, words, long.Line)
colnames(raw.Info) <- c("File Size (MB)", "Lines", "Characters", "Words", "Longest Line")
rownames(raw.Info) <- c("US Blogs", "US News", "US Twitter")
kable(raw.Info)

Rprof(NULL)
# summaryRprof(checking)
```


```{r twitterQuestions, echo = FALSE, eval=FALSE}
# The following is the analysis on Twitter

# In the en_US twitter data set, if you divide the number of lines where the word "love" (all lowercase) occurs by the number of lines the word "hate" (all lowercase) occurs, about what do you get?

love <- sum(grepl("love", raw.Twitter, ignore.case = FALSE))
hate <- sum(grepl("hate", raw.Twitter, ignore.case = FALSE))
love/hate

# The one tweet in the en_US twitter data set that matches the word "biostats" says what?
raw.Twitter[grepl("biostats", raw.Twitter, ignore.case = FALSE)]


# How many tweets have the exact characters "A computer once beat me at chess, but it was no match for me at kickboxing". (I.e. the line matches those characters exactly.)
sum(grepl("A computer once beat me at chess, but it was no match for me at kickboxing", raw.Twitter, ignore.case = TRUE))
```


Below is the top 100 words from the blogs, news and Twitter combined into a single corpus:
```{r rawDataCorpus}
# Wordcloud of the raw data combined from blogs, news and Twitter
#Rprof(corpus <- "./data/corpus.txt") #Checking for bottleneck

#create one corpus consists of blogs, news and twitter samples
raw.Corpus <- corpus(c(raw.Blogs, raw.News, raw.Twitter))
wordcloud::wordcloud(raw.Corpus, max.words = 100, random.order = FALSE,
                     rot.per=0.35, use.r.layout=TRUE, colors=brewer.pal(8, "Dark2"))
#Rprof(NULL)
```

**From the information of the data, we could see that Blogs contains highest number of words and it has the longest line. Twitter has the largest data of 319MB with the most lines and the shortest line whereas the news occupied the smalles data**

## Exploratory Data Analysis 

According to [R Blogger Site](https://www.r-bloggers.com/advancing-text-mining-with-r-and-quanteda/) after reading in the data, we need to generate a corpus. A corpus is a type of dataset that is used in text analysis. It contains “a collection of text or speech material that has been brought together according to a certain set of predetermined criteria”

Another essential component for text analysis is a data frequency matrix (DFM); also called document-term matrix (DTM). These two terms are synonyms but quanteda refers to a DFM whereas others will refer to DTM. It describes how frequently terms occur in the corpus by counting single terms.
To generate a DFM, we first split the text into its single terms (tokens). We then count how frequently each term (token) occurs in each document.

A corpus is positional (string of words) and a DFM is non-positional (bag of words). Put differently, the order of the words matters in a corpus whereas a DFM does not have information on the position of words.

A token is each individual word in a text (but it could also be a sentence, paragraph, or character). This is why we call creating a “bag of words” also tokenizing text. In a nutshell, a DFM is a very efficient way of organizing the frequency of features/tokens but does not contain any information on their position. In our example, the features of a text are represented by the columns of a DFM and aggregate the frequency of each token.

**In most projects you want one corpus to contain all your data and generate many DFMs from that.**

The rows of a DFM can contain any unit on which you can aggregate documents. In the example above, we used the single documents as the unit. It may also well be more fine-grained with sub-documents or more aggregated with a larger collection of documents.


```{r function}
# For the the purpose of text analysis, we will create two functions for dfm and ngram to apply to accomplish our task
dfm.Function <- function(corpus, n) {
        dfm(x = corpus,
            remove = dict)
}

ngram.Function <- function(corpus, n) {
        tokens_ngrams(corpus,
                      n = n)
}
```

```{r rawPlot}
# Create data for the plot
raw.Plot <- raw.Corpus %>%
              tokens(what = "word") %>%
              ngram.Function(n = 1) %>%
              dfm.Function() %>%
              topfeatures(20) %>%
              as.data.frame()
# Change column name to frequency 
colnames(raw.Plot) <- "frequency"

# Added a column to the dataframe for plotting purpose
raw.Plot$ngram <- row.names(raw.Plot)

## Generate plots for the raw data
r <- ggplot(raw.Plot, aes(y = frequency, 
                            x = reorder(ngram, frequency)))
r <- r + geom_bar(stat = "identity") + coord_flip()
r <- r + ggtitle("Top 20 Frequency of Word in the Data")
r <- r + geom_text(aes(label=frequency), 
         position = position_stack(vjust = 0.5), color="white", size=3,fontface='bold')
r <- r + ylab("Word") + xlab("Frequency")
r <- r + theme_few()
```

Below is top 20 word frequency plot:
```{r rawPlot2}
plot(r)
```

#### Initial Obersation:
From our initial observation of the raw corpus data plot and wordcloud, the data contains lots of [punctuations](https://www.thepunctuationguide.com/) and [stop words](https://en.wikipedia.org/wiki/Stop_word). From the Top 20 plot above we could see that periods and commas made up of the top 3 high frequencies in our data. The stop word "the" also rank second in the top 20 frequency and other stop words are also among the top 10.

For the next word prediction, it will not make sense to have punctuation as the next word. Therefore we need to remove punctuations when we perfom data cleaning.


## Modelling

### 2.1 Sampling and Tokenization

```{r sampling, cache=TRUE}
# Rprof(sampling <- "./data/sampling.txt") #Checking for bottleneck

set.seed(1234) #Ensure the same result for reproducibilty

sampleSize <- 0.1 #sample size is 10% of the population

sample.Blogs <- sample(raw.Blogs, size = sampleSize*length(raw.Blogs), replace = FALSE)
sample.News <-sample(raw.News, size = sampleSize*length(raw.News), replace = FALSE)
sample.Twitter <-sample(raw.Twitter, size = sampleSize*length(raw.Twitter), replace = FALSE)

# Check size
sample.Size <- round(c(object.size(sample.Blogs)/MB,
                       object.size(sample.News)/MB,
                       object.size(sample.Twitter)/MB), 2)
                       

# Number of lines in each file
sample.Lines <- c(length(sample.Blogs), 
                  length(sample.News), 
                  length(sample.Twitter))
                  

# Number of characters in each file
sample.Char <- c(sum(nchar(sample.Blogs)), 
                 sum(nchar(sample.News)), 
                 sum(nchar(sample.Twitter)))
                 

# Number of words
sample.Words <- c(wordcount(sample.Blogs, sep =" "), 
                  wordcount(sample.News, sep =" "), 
                  wordcount(sample.Twitter, sep =" "))
                  

# Longest line
sample.LongLine <- c(max(sapply(sample.Blogs, nchar)),
                     max(sapply(sample.News, nchar)),
                     max(sapply(sample.Twitter, nchar)))
                     

sample.Info <- cbind(sample.Size, sample.Lines, sample.Char, sample.Words, sample.LongLine)
colnames(sample.Info) <- c("File Size (MB)", "Lines", "Characters", "Words", "Longest Line")
rownames(sample.Info) <- c("US Blogs - Sample", "US News - Sample", "US Twitter - Sample")
kable(sample.Info)

# Rprof(NULL)
# summaryRprof(sampling)
```











Below is the top 50 words from the blogs, news and Twitter combined into a single corpus. 
```{r sampleCorpus}
#Rprof(corpus <- "./data/corpus.txt") #Checking for bottleneck


#create one corpus consists of blogs, news and twitter samples
sample.Corpus <- corpus(c(sample.Blogs,sample.News, sample.Twitter))
wordcloud::wordcloud(sample.Corpus, max.words = 50, random.order = FALSE,
                     rot.per=0.35, use.r.layout=TRUE, colors=brewer.pal(8, "Dark2"))

#head(sample.Corpus)
#length(sample.Corpus)

#Rprof(NULL)
```
The word cloud above shows that "the", "just", "like", "get

```{r profanity}

# Download profanity file from freewebheader
url_1 <- "https://www.cs.cmu.edu/~biglou/resources/bad-words.txt"
filepath_1 <- "./data/profanity_words.txt" #set the location and file name of the downloaded zip file

# Create directory named data for the file to download
if (!file.exists("./data")) {
  dir.create("./data")
}

if (!file.exists(filepath_1)){
  download.file(url_1, destfile=filepath_1)
}

profanityWords <- readLines("./data/profanity_words.txt", encoding = "UTF-8", skipNul = TRUE)
#dict <- dictionary(list(profanityWords))

```


Create tokenisation for blogs, news and Twitter using [Quanteda](https://cran.r-project.org/web/packages/quanteda/quanteda.pdf) package.



```{r dfm}
#Rprof(tokenising <- "./data/tokenising.txt") #Checking for bottleneck
# Preprocess the text

# Create tokens
sample.Token <- tokens(sample.Corpus,
                remove_numbers = TRUE,
                remove_punct = TRUE,
                remove_symbols = TRUE,
                remove_url = TRUE,
                include_docvars = TRUE)

# remove profanity words
dict <- dictionary(list(badWords = profanityWords))
sample.TokenV1  <- tokens_remove(tokens(sample.Token, dict))




sample.Dfm <- dfm(sample.TokenV1,
                  remove = dict)
head(sample.Dfm)


## Word frequency
document.Frequncy <- docfreq(sample.Dfm, scheme = "count")
#document.Frequncy

#Total number of feature
total.Features <- length(as.factor(featnames(sample.Dfm)))


# Top 20 features using topfeature function
top20.Features <- sample.Dfm %>%
                  topfeatures(20) %>%
                  head(20) %>%
                  data.frame()
#colnames(top20.Features) <- "Frequency"                  


# Top 20 features using textstat_frequency function 
tstat <- textstat_frequency(sample.Dfm, n = 20, ties_method = c("min"))


# Top 20 Textstat_keyness
tstat2 <- textstat_keyness(sample.Dfm,
                           measure = c("chi2", "exact", "lr", "pmi"),
                           sort = TRUE,
                           20)

#Token summary
tstat3 <- textstat_summary(sample.Dfm, cache = TRUE)

#Rprof(NULL)
# headsummaryRprof(tokenising)
```
After tokenising and analysis of the sample, the corpus contains `r total.Features` features and the top 20 words frequency:
`r kable(tstat, caption = "Top 20 Most Frequent Words", valign = "t")`




## 3.2 Coverage



```{r coverage}
unigram <- sample.TokenV1 %>%
            ngram.Function(n=1) %>%
            dfm.Function()
            
unigram.Frequency <- textstat_frequency(unigram)$frequency
p<-cumsum(unigram.Frequency)/sum(unigram.Frequency)

get.Coverage <- function(n = 0.5){
                which(p >=n)[1]  
}

coverage.50 <- get.Coverage(0.5)
coverage.90 <- get.Coverage(0.9)

coverages <- seq(0.1, 0.95, 0.05)

coverage.Seq <- sapply(coverages, get.Coverage)
  

cov.Df <- as.data.frame(cbind(coverages, coverage.Seq))
          

g <- ggplot(data = cov.Df, aes(y = coverages, x = coverage.Seq))
g <- g + geom_point(size = 2, colour = "red") + geom_line(size = 1, colour = "red3")
g <- g + geom_hline(yintercept = .5) + geom_hline(yintercept = .9)
g <- g + scale_y_continuous(labels = scales::percent)
g <- g + scale_x_continuous(breaks = c(coverage.50, coverage.90, get.Coverage(.95)))
g <- g + geom_vline(xintercept = coverage.50, linetype = "dashed") + geom_vline(xintercept = coverage.90, linetype = "dashed")
g <- g + ylab("Coverage %") + xlab("Number of words")
g <- g + ggtitle("Word Coverage in A Corpus")
g <- g + theme_bw()

cov.Df2 <- cov.Df
cov.Df2$coverages <- paste(cov.Df2$coverages *100, "%", sep = "")
```

The number of unique words needed to cover 50% of all word instances in the language is `r coverage.50` and the number of unique words needed to cover 90% of all words instancs in the language is `r coverage.90`.

This is table that shows the number of unique words needed to cover from 50% to 95%:
`r kable(tail(cov.Df2, n=10), col.names = c("Coverage in %", "No. of Words"), caption = "50% - 95% Unique Word Coverage", valign = "t")`

The plot below shows the unique word needed to cover up to 95% of the sampled corpora:
```{r coveragePlot, results='asis'}
plot(g)
```

```{r ngramPlot}

for (i in 1:4) {
        ## Prepare data frame for plotting
        ngram.Plot <- sample.TokenV1 %>%
                          ngram.Function(n = i) %>%
                          dfm.Function() %>%
                          topfeatures(20) %>%
                          as.data.frame()
        
        colnames(ngram.Plot) <- "frequency"
        ngram.Plot$ngram <- row.names(ngram.Plot)
        
       
         ## Generate plots 
        g <- ggplot(ngram.Plot, aes(y = frequency, 
                                      x = reorder(ngram, frequency)))
        g <- g + geom_bar(stat = "identity") + coord_flip()
        g <- g + ggtitle(paste("Top 20 - ", i, " grams", sep = " "))
        g <- g + geom_text(aes(label=frequency), 
                 position = position_stack(vjust = 0.5), color="white", size=3,fontface='bold')
        g <- g + ylab("") + xlab("")
        g <- g + theme_few()
        assign(paste("p", i, sep = ""), g)
}

##grid.arrange(p1, p2, p3, p4, nrow=2, ncol=2, top ="Top 20 ngrams")
```

Side by Side comparison of bigram, trigram and quadgram wordcloud.
```{r WordcloudNgram, echo = FALSE, warning=FALSE, results='asis'}

par(mfrow=c(1,3))
biCloud <- sample.TokenV1 %>%
            ngram.Function(n=2) %>%
            dfm.Function() %>%
            textplot_wordcloud(max_words = 50,colors=brewer.pal(8, "Dark2"))

triCloud <- sample.TokenV1 %>%
            ngram.Function(n=3) %>%
            dfm.Function() %>%
            textplot_wordcloud(max_words = 50,colors=brewer.pal(8, "Dark2"))

quadCloud <- sample.TokenV1 %>%
            ngram.Function(n=4) %>%
            dfm.Function() %>%
            textplot_wordcloud(max_words = 50,colors=brewer.pal(8, "Dark2"))
```



### Appendix Code
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```

## References & Sources:
Natural Language Processing Stanford Uni by Jurafsky & Manning - https://www.youtube.com/watch?v=oWsMIW-5xUc&list=PLLssT5z_DsK8HbD2sPcUIDfQ7zmBarMYv

N-Gram Models by Durafsky & Martin - https://web.stanford.edu/~jurafsky/slp3/3.pdf

Text Analysis in R by Welbers, Atteveldt & Benoit - https://kenbenoit.net/pdfs/text_analysis_in_R.pdf

Basic Text Analysis in R by Bail - https://compsocialscience.github.io/summer-institute/2018/materials/day3-text-analysis/basic-text-analysis/rmarkdown/Basic_Text_Analysis_in_R.html

Introduction to Text Analytics in R by Langer - https://www.youtube.com/playlist?list=PL8eNk_zTBST8olxIRFoo0YeXxEOkYdoxi

Cran Task View: Natural Language Processing by Fridolin Wild - https://cran.r-project.org/web/views/NaturalLanguageProcessing.html

Text Mining Infrastructure in R by Ingo, Kurt & David - https://www.researchgate.net/publication/26539008_Text_Mining_Infrastructure_in_R

Guide to Ngram Package by Schmidt- https://cran.r-project.org/web/packages/ngram/vignettes/ngram-guide.pdf

Text Mining in R: A Tidy Approach, by Julia Silge and David Robinson - https://www.tidytextmining.com/index.html

Text Mining and Wordcloud Fundemental in R - http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know

textstat_frequency: Tabulate feature frequencies - https://rdrr.io/cran/quanteda/man/textstat_frequency.html

Predicting the Next Word: Back-Off Language Modeling by Masse  - https://medium.com/@davidmasse8/predicting-the-next-word-back-off-language-modeling-8db607444ba9

Natural language processing: What would Shakespeare say? by Ganesh - https://www.r-bloggers.com/natural-language-processing-what-would-shakespeare-say/

Natural Language processing in Python by Zhao - https://youtu.be/xvqsFTUsOmc