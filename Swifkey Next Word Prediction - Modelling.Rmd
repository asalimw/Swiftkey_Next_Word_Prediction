---
title: "Swiftkey Next Word Prediction - Modelling"
author: "Willianto Asalim"
date: "01/09/2020"
output: html_document
---


```{r LoadPackages, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr) ##Load Knitr package
library(ggplot2) ##Plotting and data
library(caret) ##Load package for ML
library(dplyr) ##Data transformation package
library(quanteda)
library(ngram) ## 
library(tm)
library(RColorBrewer)
library(ggthemes)
library(gridExtra)
library(tidytext)
library(wordcloud)
library(markovchain)
library(tidyr)
library(data.table)
library(tidyr)
```

```{r setoptions, echo=FALSE}
## Setting Global Option where echo = true so that someone will be able to read the code and results.
knitr::opts_chunk$set(echo = FALSE, results = "hold", tidy = TRUE)
```

## Load Relevant Data
```{r loadRelevantData}
load(file = "./data/sampleTokenV1.rda")
load(file = "./data/sampleToken.rda")
load(file = "./data/sampleCorpus.rda")
```

ice that a small fraction (less than 50%) of unique words accounts for the majority of text. To improve the speed of the our model we could use unique words with less than 50% coverage. 

As stated by Dan Durafsky in [Interpolation](https://www.youtube.com/watch?v=naNezonMA7k) part of NLP, the best method to use for for very large N-grams like the web would be to use the "Stupid Backoff" method. 

Credit to "NLP: What would Shakepeare say?" blog by [Trinviam V Ganesh](https://www.r-bloggers.com/natural-language-processing-what-would-shakespeare-say/) for the guide to implement the modelling. Our fellow member Paul Ringsted in the discussion for giving the tips and pointers on how to proceed with this.

Credit to Michael Szcepaniak from the forum for providing the   [Katz Backoff Formula](https://raw.githubusercontent.com/MichaelSzczepaniak/DSSCapstoneMentor/master/kbot_complete_hand_written_example.pdf)



```{r profanity}
# Profanity word filter

# Download profanity file from freewebheader
url_1 <- "https://www.cs.cmu.edu/~biglou/resources/bad-words.txt"
filepath_1 <- "./data/profanity_words.txt" #set the location and file name of the downloaded zip file

# Create directory named data for the file to download
if (!file.exists("./data")) {
  dir.create("./data")
}

if (!file.exists(filepath_1)){
  download.file(url_1, destfile=filepath_1)
}

profanityWords <- readLines("./data/profanity_words.txt", encoding = "UTF-8", skipNul = TRUE)
dict.Profanity <- dictionary(list(badWord = profanityWords))

```

## Sample Data
```{r CreateData}
# remove the special characters that might indicate "Twitter" or other social media conventions.
sample.CorpusV2 <- sample.Corpus %>%
                   stri_replace_all_regex("[\\p{p}\\p{S}]", "") %>%   #remove all punctuation and symbols
                   stri_replace_all_regex("(\\b)_(\\w+)", "$1$2") %>% #how to remove the leading _
                  stri_replace_all_regex("\\d", "") #remove all digits

# Convert to tokens
sample.Token <- tokens(sample.CorpusV2,
                remove_numbers = TRUE,
                remove_punct = TRUE,
                remove_symbols = TRUE,
                remove_url = TRUE,
                include_docvars = TRUE)

# remove profanity words
sample.TokenV1  <- tokens_remove(tokens(sample.Token, dict.Profanity))

```
From the coverage plot we also not

```{r function}
# For the the purpose of text analysis, we will create two functions for dfm and ngram to apply to accomplish our task
dfm.Function <- function(corpus, n) {
        dfm(x = corpus,
            remove = dict.Profanity)
}

ngram.Function <- function(corpus, n) {
        tokens_ngrams(corpus,
                      n = n)
}
```



## ngram
```{r ngramCreate}
for (i in 1:4) {
        ## Create unigram, bigram, trigram and quadgram data table
        ngram <- sample.TokenV1 %>%
                          ngram.Function(n = i)
        #assign name to the ngram ie. unigram as ngram1, bigram as gram2
        assign(paste("ngram", i, sep = ""), ngram)
} 

#cleaning duplicate 
rm(ngram)
```



##Create DMF data table (manual)
```{r createDFM}
dfm1 <- ngram1 %>%
        dfm.Function()


dfm2 <- ngram2 %>%
        dfm.Function()
        
dfm3 <- ngram3 %>%
        dfm.Function()

dfm4 <- ngram4 %>%
        dfm.Function()
```



#data table

```{r dataTable}
unigram.Dt <- data.table(ngram = featnames(dfm1), 
                  count = colSums(dfm1), 
                  frequency = docfreq(dfm1), 
                  coverage = cumsum(docfreq(dfm1))/sum(docfreq(dfm1)), 
                  key = "ngram")

bigram.Dt <- data.table(ngram = featnames(dfm2), 
                  count = colSums(dfm2), 
                  frequency = docfreq(dfm2), 
                  coverage = cumsum(docfreq(dfm2))/sum(docfreq(dfm2)),
                  key = "ngram")

trigram.Dt <- data.table(ngram = featnames(dfm3), 
                  count = colSums(dfm3), 
                  frequency = docfreq(dfm3), 
                  coverage = cumsum(docfreq(dfm3))/sum(docfreq(dfm3)),
                  key = "ngram")  

quadgram.Dt <- data.table(ngram = featnames(dfm4), 
                  count = colSums(dfm4), 
                  frequency = docfreq(dfm4), 
                  coverage = cumsum(docfreq(dfm4))/sum(docfreq(dfm4)),
                  key = "ngram")
```



## Clean Data table
To improve the performance of our prediction we will reduce the size to cover only word with less than 50% coverage in the data table
```{r cleanDataTable}
# Remove word with more than 50% coverage in the data table
unigram.Clean <- unigram.Dt[!(unigram.Dt$coverage>.5)]
bigram.Clean <- bigram.Dt[!(bigram.Dt$coverage>.5)]
trigram.Clean <- trigram.Dt[!(trigram.Dt$coverage>.5)]
quadgram.Clean <- quadgram.Dt[!(quadgram.Dt$coverage>.5)]
```

## Sorted Data Table

```{r SortDataTable}
unigram.sort <- unigram.Clean[order(-frequency, -coverage)]
bigram.sort <- bigram.Clean[order(-frequency, -coverage)]
trigram.sort <- trigram.Clean[order(-frequency, -coverage)]
quadgram.sort <- quadgram.Clean[order(-frequency, -coverage)]

```


## Separate Words
```{r separateWords}

biWords <- bigram.Clean %>%
            separate(ngram, c("word1", "word2"), sep = "_")

triWords <- trigram.Clean %>%
            separate(ngram, c("word1", "word2", "word3"), sep = "_")

quadWords <- quadgram.Clean %>%
            separate(ngram, c("word1", "word2", "word3", "word4"), sep = "_")
```

```{r SaveWords}

if (!file.exists("./data/clean")) {
  dir.create("./data/clean")
}

save(biWords, file = "./data/clean/biWords.rda")
save(triWords, file = "./data/clean/triWords.rda")
save(quadWords, file = "./data/clean/quadWords.rda")
```

Quiz 2
DO NOT USE
```{r quiz2}

#' Create Ngram Matching Functions
bigram <- function(input_words){
                    num <- length(input_words)
                    filter(biWords, 
                          word1==input_words[num]) %>% 
                    top_n(1, n) %>%
                    filter(row_number() == 1L) %>%
                    select(num_range("word", 2)) %>%
                    as.character() -> out
                    ifelse(out =="character(0)", "?", return(out))
}

trigram <- function(input_words){
                    num <- length(input_words)
                    filter(triWords, 
                            word1==input_words[num-1], 
                            word2==input_words[num])  %>% 
                    top_n(1, n) %>%
                    filter(row_number() == 1L) %>%
                    select(num_range("word", 3)) %>%
                    as.character() -> out
                    ifelse(out=="character(0)", bigram(input_words), return(out))
}

quadgram <- function(input_words){
                    num <- length(input_words)
                    filter(quadWords, 
                            word1==input_words[num-2], 
                            word2==input_words[num-1], 
                            word3==input_words[num])  %>% 
                    top_n(1, n) %>%
                    filter(row_number() == 1L) %>%
                    select(num_range("word", 4)) %>%
                    as.character() -> out
                    ifelse(out=="character(0)", trigram(input_words), return(out))
}

predictWord <- function(input){
  # Create a dataframe
  input <- data_frame(text = input)
  # Clean the Inpput
  replace_reg <- "[^[:alpha:][:space:]]*"
  input <- input %>%
    mutate(text = str_replace_all(text, replace_reg, ""))
  # Find word count, separate words, lower case
  input_count <- str_count(input, boundary("word"))
  input_words <- unlist(str_split(input, boundary("word")))
  input_words <- tolower(input_words)
  # Call the matching functions
  out <- ifelse(input_count == 1, bigram(input_words), 
              ifelse (input_count == 2, trigram(input_words), quadgram(input_words)))
  # Output
  return(out)
}
```



## Markov Chain Fit
