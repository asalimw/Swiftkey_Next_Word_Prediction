---
title: "Swiftkey_Next_Word_Prediction"
author: "Willianto Asalim"
date: "18/08/2020"
output: html_document
---

##### The platform specification used:
Spec    | Description
------- | -----------------------
OS      | Windows 10 Pro - 64 bit
CPU     | AMD Ryzen 5 - 3400G (4 cores & 8 threads)
RAM     | 16GB DDR4 3000MHz
Storage | 500GB SSD - M.2 NVMe (PCIe) 
Tool    | RStudio


```{r LoadPackages, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr) ##Load Knitr package
library(ggplot2) ##Plotting and data
library(caret) ##Load package for ML
library(dplyr) ##Data transformation package
library(quanteda)
library(ngram) ## 
```

```{r setoptions, echo=FALSE}
## Setting Global Option where echo = true so that someone will be able to read the code and results.
knitr::opts_chunk$set(echo = TRUE, results = "hold", tidy = TRUE)
```

# Swiftkey - Next Word Prediction

## Background
Around the world, people are spending an increasing amount of time on their mobile devices for email, social networking, banking and a whole range of other activities. But typing on mobile devices can be a serious pain. SwiftKey, our corporate partner in this capstone, builds a smart keyboard that makes it easier for people to type on their mobile devices. One cornerstone of their smart keyboard is predictive text models. When someone types:

I went to the

the keyboard presents three options for what the next word might be. For example, the three words might be gym, store, restaurant. In this capstone you will work on understanding and building predictive text models like those used by SwiftKey.

The first step in analyzing any new data set is figuring out: (a) what data you have and (b) what are the standard tools and models used for that type of data. This exercise uses the files named LOCALE.blogs.txt where LOCALE is the each of the four locales en_US, de_DE, ru_RU and fi_FI. The data is from a corpus called HC Corpora. The files have been language filtered but may still contain some foreign text.

As a first step toward working on this project is to familiarize Natural Language Processing, Text Mining, and the associated tools in R. 


### Dataset
This is the training data to get started that will be the basis for most of the capstone. Download the data from the link below and not from external websites to start.

[HC Corpora Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)

The corpora are collected from publicly available sources by a web crawler. The crawler checks for language, so as to mainly get texts consisting of the desired language. Each entry is tagged with it's date of publication. Where user comments are included they will be tagged with the date of the main entry.

Each entry is tagged with the type of entry, based on the type of website it is collected from (e.g. newspaper or personal blog) If possible, each entry is tagged with one or more subjects based on the title or keywords of the entry (e.g. if the entry comes from the sports section of a newspaper it will be tagged with "sports" subject).In many cases it's not feasible to tag the entries (for example, it's not really practical to tag each individual Twitter entry, though I've got some ideas which might be implemented in the future) or no subject is found by the automated process, in which case the entry is tagged with a '0'.

Once the raw corpus has been collected, it is parsed further, to remove duplicate entries and split into individual lines. Approximately 50% of each entry is then deleted. 


## Task 1: Getting and Cleaning Data
Download the data on the the directory specified and unzip the file at the same time
```{r downloadData}
# URL is the given link to download from HC Corpora dataset
data.Url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
filepath <- "./data/SwiftKey.zip" #set the location and file name of the downloaded zip file

# Create directory named data for the file to download
if (!file.exists("./data")) {
  dir.create("./data")
}

if (!file.exists(filepath)){
  download.file(url, destfile=filepath, method="curl")
  unzip(zipfile=filepath, exdir="./data") #Unzip the file and store the folder in the data folder
}
```


```{r readRawData}
# Assign English data location
loc.Blogs <- "./data/final/en_US/en_US.blogs.txt"
loc.News <- "./data/final/en_US/en_US.news.txt"
loc.Twitter <- "./data/final/en_US/en_US.twitter.txt"

# Read the data files
raw.Blogs <- readLines(loc.Blogs, encoding = "UTF-8", skipNul = TRUE)
raw.News <- readLines(loc.News, encoding = "UTF-8", skipNul = TRUE, warn = FALSE)
raw.Twitter <- readLines(loc.Twitter, encoding = "UTF-8", skipNul = TRUE)
```

```{r checkFile}
Rprof(checking <- "./data/checking.txt") #Checking for bottleneck

MB <- 2^20 #Byte to MB conversion

# Check File Size
size <- round(c(object.size(raw.Blogs)/MB,
                object.size(raw.News)/MB,
                object.size(raw.Twitter)/MB),2)

# Number of lines in each file
lines <- c(length(raw.Blogs), 
           length(raw.News), 
           length(raw.Twitter))

# Number of characters in each file
char <- c(sum(nchar(raw.Blogs)), 
          sum(nchar(raw.News)), 
          sum(nchar(raw.Twitter)))

# Number of words
words <- c(wordcount(raw.Blogs, sep =" "), 
           wordcount(raw.News, sep =" "), 
           wordcount(raw.Twitter, sep =" "))

# Longest line
long.Line <- c(max(sapply(raw.Blogs, nchar)),
               max(sapply(raw.News, nchar)),
               max(sapply(raw.Twitter, nchar)))

raw.Info <- cbind(size, lines, char, words, long.Line)
colnames(raw.Info) <- c("File Size (MB)", "Lines", "Characters", "Words", "Longest Line")
rownames(raw.Info) <- c("US Blogs", "US News", "US Twitter")
kable(raw.Info)

Rprof(NULL)
# summaryRprof(checking)
```

```{r twitterQuestions, echo = FALSE, eval=FALSE}
# In the en_US twitter data set, if you divide the number of lines where the word "love" (all lowercase) occurs by the number of lines the word "hate" (all lowercase) occurs, about what do you get?

love <- sum(grepl("love", raw.Twitter, ignore.case = FALSE))
hate <- sum(grepl("hate", raw.Twitter, ignore.case = FALSE))
love/hate

# The one tweet in the en_US twitter data set that matches the word "biostats" says what?
raw.Twitter[grepl("biostats", raw.Twitter, ignore.case = FALSE)]


# How many tweets have the exact characters "A computer once beat me at chess, but it was no match for me at kickboxing". (I.e. the line matches those characters exactly.)
sum(grepl("A computer once beat me at chess, but it was no match for me at kickboxing", raw.Twitter, ignore.case = TRUE))
```


## Task2: Exploratory Data Analysis

The first step in building a predictive model for text is understanding the distribution and relationship between the words, tokens, and phrases in the text. The goal of this task is to understand the basic relationships you observe in the data and prepare to build your first linguistic models.

Tasks to accomplish

1. Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora.

2. Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.

Given the huge scale of more than 500MB of data and It is inefficient and impractical to explore and clean all of the data. It will take a lot of computing power and time to perform all the required task to accomplish the objective. Therefore we will take a sample size from each En_US text file and combined them into a single corpora. A good maximum sample size is usually around 10% of the population, For example, in a population of 5000, 10% would be 500. In a population of 200,000, 10% would be 20,000. Therefore we will need create samples for the blogs, news and twitter dataset.

2.1 Sampling and Tokenization
```{r sampling, cache=TRUE}
Rprof(sampling <- "./data/sampling.txt") #Checking for bottleneck

set.seed(1234) #Ensure the same result for reproducibilty

sampleSize <- 0.1 #sample size is 10% of the population

sample.Blogs <- sample(raw.Blogs, size = sampleSize*length(raw.Blogs), replace = FALSE)
sample.News <-sample(raw.News, size = sampleSize*length(raw.News), replace = FALSE)
sample.Twitter <-sample(raw.Twitter, size = sampleSize*length(raw.Twitter), replace = FALSE)

#create one corpus consists of blogs, news and twitter samples
sample.Corpus <- corpus(c(sample.Blogs,sample.News, sample.Twitter))

# Check size
sample.Size <- round(c(object.size(sample.Blogs)/MB,
                       object.size(sample.News)/MB,
                       object.size(sample.Twitter)/MB),
                       object.size(sample.Corpus)/MB, 2)

# Number of lines in each file
sample.Lines <- c(length(sample.Blogs), 
                  length(sample.News), 
                  length(sample.Twitter),
                  length(sample.Corpus))

# Number of characters in each file
sample.Char <- c(sum(nchar(sample.Blogs)), 
                 sum(nchar(sample.News)), 
                 sum(nchar(sample.Twitter)),
                 sum(nchar(sample.Corpus)))

# Number of words
sample.Words <- c(wordcount(sample.Blogs, sep =" "), 
                  wordcount(sample.News, sep =" "), 
                  wordcount(sample.Twitter, sep =" "),
                  wordcount(sample.Corpus, sep = " "))

# Longest line
sample.LongLine <- c(max(sapply(sample.Blogs, nchar)),
                     max(sapply(sample.News, nchar)),
                     max(sapply(sample.Twitter, nchar)),
                     max(sapply(sample.Corpus, nchar)))

sample.Info <- cbind(sample.Size, sample.Lines, sample.Char, sample.Words, sample.LongLine, sample.Corpus)
colnames(sample.Info) <- c("File Size (MB)", "Lines", "Characters", "Words", "Longest Line")
rownames(sample.Info) <- c("US Blogs - Sample", "US News - Sample", "US Twitter - Sample", "Corpus - Sample")
kable(sample.Info)

Rprof(NULL)
# summaryRprof(sampling)
```

After reading in the data, we need to generate a corpus. A corpus is a type of dataset that is used in text analysis. It contains “a collection of text or speech material that has been brought together according to a certain set of predetermined criteria”

Another essential component for text analysis is a data frequency matrix (DFM); also called document-term matrix (DTM). These two terms are synonyms but quanteda refers to a DFM whereas others will refer to DTM. It describes how frequently terms occur in the corpus by counting single terms.
To generate a DFM, we first split the text into its single terms (tokens). We then count how frequently each term (token) occurs in each document.

A corpus is positional (string of words) and a DFM is non-positional (bag of words). Put differently, the order of the words matters in a corpus whereas a DFM does not have information on the position of words.

A token is each individual word in a text (but it could also be a sentence, paragraph, or character). This is why we call creating a “bag of words” also tokenizing text. In a nutshell, a DFM is a very efficient way of organizing the frequency of features/tokens but does not contain any information on their position. In our example, the features of a text are represented by the columns of a DFM and aggregate the frequency of each token.

In most projects you want one corpus to contain all your data and generate many DFMs from that.

The rows of a DFM can contain any unit on which you can aggregate documents. In the example above, we used the single documents as the unit. It may also well be more fine-grained with sub-documents or more aggregated with a larger collection of documents.

```{r corpus}
Rprof(corpus <- "./data/corpus.txt") #Checking for bottleneck

sample.Corpus <- corpus(c(sample.Blogs,sample.News, sample.Twitter))
head(sample.Corpus)
length(sample.Corpus)

Rprof(NULL)
```


Create tokenisation for blogs, news and Twitter using [Quanteda](https://cran.r-project.org/web/packages/quanteda/quanteda.pdf) package.

```{r tokenisation}
Rprof(tokenising <- "./data/tokenising.txt") #Checking for bottleneck

token.Blogs <- tokens(sample.Blogs, 
                      what = "word", 
                      remove_punct = TRUE, 
                      remove_symbols = TRUE, 
                      remove_numbers = TRUE, 
                      remove_url = TRUE,
                      remove_separators = TRUE,
                      verbose = FALSE)

token.News <- tokens(sample.News,
                     what = "word", 
                     remove_punct = TRUE, 
                     remove_symbols = TRUE, 
                     remove_numbers = TRUE, 
                     remove_url = TRUE,
                     remove_separators = TRUE,
                     verbose = FALSE)

token.Twitter <- tokens(sample.Twitter,
                        what = "word", 
                        remove_punct = TRUE, 
                        remove_symbols = TRUE, 
                        remove_numbers = TRUE, 
                        remove_url = TRUE,
                        remove_separators = TRUE,
                        verbose = FALSE)
length(token.Blogs)
length(token.News)
length(token.Twitter)

Rprof(NULL)
# headsummaryRprof(tokenising)
```


### Appendix Code
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```

## References & Sources:
Natural Language Processing Stanford Uni by Jurafsky & Manning - https://www.youtube.com/watch?v=oWsMIW-5xUc&list=PLLssT5z_DsK8HbD2sPcUIDfQ7zmBarMYv

N-Gram Models by Durafsky & Martin - https://web.stanford.edu/~jurafsky/slp3/3.pdf

Introduction to Text Analytics in R by Langer - https://www.youtube.com/playlist?list=PL8eNk_zTBST8olxIRFoo0YeXxEOkYdoxi

Cran Task View: Natural Language Processing by Fridolin Wild - https://cran.r-project.org/web/views/NaturalLanguageProcessing.html

Text Mining Infrastructure in R by Ingo, Kurt & David - https://www.researchgate.net/publication/26539008_Text_Mining_Infrastructure_in_R

Guide to Ngram Package by Schmidt- https://cran.r-project.org/web/packages/ngram/vignettes/ngram-guide.pdf

R Programming for Dummies - https://www.dummies.com/programming/r/

Text Mining in R: A Tidy Approach, by Julia Silge and David Robinson - https://www.tidytextmining.com/index.html